{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "Tim Repke, Julian Risch\n",
    "\n",
    "July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Some initial notes (not part of presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks\n",
    "- point\n",
    "- point\n",
    "- point\n",
    "- point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "some more on NN basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Formal Definition\n",
    "$y=wx+b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "some more math in notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do they learn?\n",
    "- output neurons\n",
    "- error \"diff\" function (loss, objective,...)\n",
    "- error back-propagated using gradient\n",
    "- weights and biases updated accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Types of Neural Networks\n",
    "- **ANN/NN:** Artificial Neural Networks *(1D input, what we just saw)*\n",
    "  - good for 1D input classification or regression\n",
    "  - learning embeddings (reduction into lower dimensional \"semantic\" space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **CNN:** Convolutional Neural Networks *(2D input)*\n",
    "  - 2D input classification; ideal for images, sometimes *fixed* length sequences\n",
    "  - image segmentation (masking areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **RNN:** Recurrent Neural Networks *(sequential input)*\n",
    "  - sequence to sequence models (i.e. translation, tagging, summarisation, stock prognosis)\n",
    "  - sequence to scalar also possible (i.e. classifying entire sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks - Hands on!\n",
    "First, we load some general dependencies we may need all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Dataset\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "path = get_file(os.getcwd()+'/datasets/reuters.npz', origin='https://s3.amazonaws.com/text-datasets/reuters.npz')\n",
    "with np.load(path) as f:\n",
    "    reuters_data, labels = f['x'], f['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of samples:', reuters_data.shape)\n",
    "print('Part of the first sample:', reuters_data[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why are there only numbers? Isn't that supposed to be text? Indeed it is, but encoded by a dictionary, which we can load as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "path = get_file(os.getcwd()+'/datasets/reuters_word_index.json', origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.json')\n",
    "with open(path) as f:\n",
    "    reuters_words = json.load(f)\n",
    "    reuters_words_inverse = {v:k for k,v in reuters_words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dictionary size:\", len(reuters_words))\n",
    "print(\"Word index:\")\n",
    "pprint(dict(list(reuters_words.items())[:5]))\n",
    "print(\"\\nReverse word index:\")\n",
    "pprint(dict(list(reuters_words_inverse.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's \"decode\" some of those sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([reuters_words_inverse[wi] for wi in reuters_data[42]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Getting to know the data is important.\n",
    "One interesting aspect is to look into the distribution of labels.\n",
    "A bias might require special attention during training.\n",
    "It also means, that we need to consider a strong bias in the evaluation later on, especially when creating the train/test split!\n",
    "You'll see some parameters later on, that lead back to the distribution of labels, i.e. `stratified` and `balanced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# a small pandas hack for \"simplicity\"\n",
    "label_distribution = pd.DataFrame({'labels':labels}).groupby('labels').labels.count()\n",
    "\n",
    "# make a barplot\n",
    "label_distribution.plot.bar(figsize=(12,5))\n",
    "\n",
    "# look at the bare numbers\n",
    "label_distribution.to_frame().T # remove the '.T' to see the full list vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Feel free to play with the data. \n",
    "- What's the distribution of words?\n",
    "- What's the distribution of words in different topics?\n",
    "- How long are the text samples?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### The easier way\n",
    "Keras has also some [built in](https://keras.io/datasets/) data loading functions that essentially do what we did above.\n",
    "That was just to show you how to go about things when you have you own data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "max_words = 50\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preparing the dataset\n",
    "There are two things we look at in this section. \n",
    "\n",
    "**Train/Test split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We split our data into two subsets.\n",
    "One, the bigger chunk, we use for training the mode, the other for testing it on.\n",
    "This way we have some data the model has never seen before.\n",
    "If it performs reasonable good classifying those, we can use it as an indicator, that it can generalise to other examples as well.\n",
    "However, there are several aspects one should consider depending on the dataset and task, like overlapping concepts or data.\n",
    "Here, we neglect all that for simplicity though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "At it's core, a neural network, as any other machine learning algorithm, is pure math. Text isn't. Thus we need to encode it in some way as numerical input as described down below.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Subsampling** data down to 4 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We just do that here in the example to make things easier and the evaluation less complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = [4,11,16,19]\n",
    "selected_labels_index = {l:i for i,l in enumerate(selected_labels)}\n",
    "selection = [l in selected_labels for l in labels]\n",
    "reuters_data_sampled = reuters_data[selection]\n",
    "labels_sampled = labels[selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Original size of dataset:', len(labels))\n",
    "print('Size of subset for',len(selected_labels),'labels:', len(labels_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Splitting the data** using scikit-learn [utility functions](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note the parameter `stratify`, to which we hand our labels. This tells the function to not do the sampling completely random, but to make sure that samples from all classes are in both sets, as well as their relative distribution is about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reuters_data_sampled, labels_sampled,\n",
    "                                                    test_size=0.30, random_state=42,\n",
    "                                                    stratify=labels_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Size training set:', len(X_train))\n",
    "print('Size testing set:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Usually you would split your data into three parts:\n",
    "- one for **train**ing (usually the biggest chunk),\n",
    "- one for development **test**ing, \n",
    "- and one for a final **eval**uation.\n",
    "\n",
    "The last one you never look at or touch during your development and experiments. \n",
    "Only at the very end you will test the performance of what you determined to be your best model on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Encoding** the text-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In our first example with a simple neural network, we use a simple [bag-of-words (BoW)](https://en.wikipedia.org/wiki/Bag-of-words_model) encoding for our text.\n",
    "\n",
    "Keras provides [utility function](https://keras.io/preprocessing/text/#tokenizer), which we use to encode the text.\n",
    "It takes the first `max_words` entries in the dictionary and for each sample creates a vector of that length. \n",
    "Each position in that vector corresponds to a word's index in the dictionary and is set to 1 if it appears in the sample, 0 if not.\n",
    "In other modes, instead of setting a 1, some other score is used (i.e. number of occurences in the sample).\n",
    "\n",
    "Hints for experiments:\n",
    "- Try different vocab sizes\n",
    "- Try different modes\n",
    "- Look into the [implementation](https://github.com/fchollet/keras/blob/master/keras/preprocessing/text.py#L297) and do something like dropping stopwords (extremely frequently used words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words=5000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "X_train_tokenised = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "X_test_tokenised = tokenizer.sequences_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape, '| X_train_tokenised shape:', X_train_tokenised.shape)\n",
    "print('X_test  shape:', X_test.shape,  '| X_test_tokenised  shape:', X_test_tokenised.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Encoding** the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We interpret the output of the last layer of our network as the predicted label.\n",
    "We could naively assume, that we use one neuron that outputs the number to the corresponding class.\n",
    "However, it has proven to be significantly better to one-hot encode the labels, such that we have as many output neurons in the last layer as unique labels.\n",
    "Each output neuron corresponds to one label.\n",
    "Using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), we can interpret the out as a confidence score for that label - the higher (closer to 1), the more confident the network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_ = np.array([selected_labels_index[ty] for ty in y_train])\n",
    "y_test_ = np.array([selected_labels_index[ty] for ty in y_test])\n",
    "Y_train = to_categorical(y_train_, len(selected_labels))\n",
    "Y_test = to_categorical(y_test_, len(selected_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building the Neural Network\n",
    "One hidden layer with 512 neurons with ReLU [activation](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions); Softmax activation at the output layer; Optimisation using [Adam](https://arxiv.org/abs/1412.6980v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(selected_labels)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hints for experiments:\n",
    "- Try adding another layer\n",
    "- Check out the [documentation](https://keras.io/layers/core/)\n",
    "- Use different sizes of hidden layers\n",
    "- Use different weight initialisations\n",
    "- Use different [activation](https://keras.io/activations/) functions\n",
    "- Use different (or no) dropout\n",
    "- Use different [loss](https://keras.io/losses/) functions\n",
    "- Use different [optimisers](https://keras.io/optimizers/)\n",
    "\n",
    "Below you see a different syntax with some additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(max_words, activation='linear', input_dim=max_words, \n",
    "          kernel_initializer='he_uniform'),\n",
    "    Dense(2048, activation='relu', bias_regularizer=l2(0.01),\n",
    "          kernel_initializer='he_uniform', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(selected_labels), activation='softmax',\n",
    "          kernel_initializer='he_uniform')\n",
    "])\n",
    "\n",
    "model.compile(loss='msle',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Class weight computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As we have seen earlier, the distribution of samples across classes is somewhat unbalanced.\n",
    "This unbalance potentially causes to bias our classification model and there are different approaches to counteract that.\n",
    "One is to use weights during training, such that the error of samples from over-represented classes is virtually reduced by a factor and vice-versa amplified for samples of under-represented classes.\n",
    "For further reading, you could look up keywords like over- or under-sampling as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = {key: value for key, value in enumerate(\n",
    "                     compute_class_weight('balanced',\n",
    "                                          np.arange(len(selected_labels)),\n",
    "                                          np.array(y_train_)))}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are quite a few things going on here!\n",
    "Epochs sets the number of times we let the network see the entire training set.\n",
    "The batch size adjusts a number of things.\n",
    "Essentially, it grabs a batch of samples, does the forward passes, and only then does the back-propagation based on those gradients.\n",
    "This saves time, compared to calculating gradients and updating all variables for each training sample.\n",
    "The class weight can be used to amplify the error for samples from underrepresented classes.\n",
    "The validation split sets a small subset of *training* samples aside for testing along the training run.\n",
    "If we had a proper train/test/eval split, we would use the test split for that (or a portion for that, because calculating these metrics takes time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "history = model.fit(X_train_tokenised, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    #class_weight=class_weights,\n",
    "                    validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Keras has a simple evaluation function built into the model, which can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_tokenised, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We want to dig much deeper, so below you'll find a number of helpful [scikit-learn functions](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get model predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test_tokenised)\n",
    "y_pred = y_pred_proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **classification report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For each class, it shows the precision, recall and their harmonic mean (F1-score).\n",
    "The support shows, how many samples are in that particular class (based on test labels).\n",
    "We also calculate the accuracy.\n",
    "Both functions also allow normalisation to correct for class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_, y_pred, target_names=['Topic '+str(sl) for sl in selected_labels]))\n",
    "print(\"Accuracy: {0:.3f}\".format(accuracy_score(y_test_, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **confusion matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Based on the cofusion matrix you can figure out how the trained model makes mistakes. \n",
    "Vertically you can imagine the label of the correct class, horizontally that of the predicted class. \n",
    "The perfect classifier would only contain non-zero numbers along the diagonal.\n",
    "Considering the first row, you will see for samples of that topic, with what other topic they are confused with most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test_, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To get a more visually appealing result, you need a more code. Note that the normalisation was done *per class* for convenience, not globally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import cm as colm\n",
    "import matplotlib\n",
    "\n",
    "conf_matrix_full = confusion_matrix(y_test_, y_pred, labels=list(range(len(selected_labels))))\n",
    "\n",
    "matplotlib.rc('font', **{'size':12})\n",
    "plt.figure(figsize=(6,4))\n",
    "sub = plt.subplot(111)\n",
    "normed = (conf_matrix_full.T/conf_matrix_full.sum(axis=1)).T\n",
    "plt.imshow(normed, cmap=colm.Blues, vmax=normed.max()*1.4)\n",
    "sub.set_yticks(list(range(len(selected_labels))))\n",
    "sub.set_yticklabels(['Topic '+str(sl) for sl in selected_labels])\n",
    "sub.set_xticks(list(range(len(selected_labels))))\n",
    "sub.set_xticklabels(['Topic '+str(sl) for sl in selected_labels])\n",
    "\n",
    "for i in range(normed.shape[0]):\n",
    "    for j in range(normed.shape[1]):\n",
    "        v = normed.T[i][j]\n",
    "        c='%.2f'%v if v>0.005 else ''\n",
    "        sub.text(i, j, c, va='center', ha='center')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Area Under the Curve** and **Precision vs Recall curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As described earlier, we have an output neuron per class and interpret it's activation as the confidence.\n",
    "Above we made the simple assumption to just take the class where the neuron has the highest activation for each sample.\n",
    "We can also look at the precision recall curve, where we slowly increase the threshold of the confidence score we accept.\n",
    "Obviously, when we want a very high confidence close to 1, the precision will be (hopefully) very good.\n",
    "However, we may loose a lot of samples where there's more uncertainty, and thus the recall drops.\n",
    "\n",
    "The **precision recall curve describes the tradeoff** we can take.\n",
    "In the code below, you can print the variables returned by the sklearn function to see the range of thresholds.\n",
    "The AUC score essentially is just the area under the curve just described.\n",
    "A desireable model will have a high AUC and therefore the curve mostly high and stable.\n",
    "The perfect classifier which never fails would have a constant precision of 1 for all recall values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "plt.clf()\n",
    "for i, f in enumerate(['Topic '+str(sl) for sl in selected_labels]):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test_ == i, y_pred_proba[:, i])\n",
    "    auc_ = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=\"{}, AUC={:.4}\".format(f, auc_))\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.xlim(0, 1.1)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **training curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Remember that `history` variable we stored earlier during training? That contains all the values you see when you train the model with the verbose flag set to 1. We can also visualise it for convenience.\n",
    "\n",
    "Things to look out for are as follows: The **loss** is the value the optimiser aims to minimise *(also called objective, error,...)*. It describes, based on the loss function, how far off the current prediction is from the correct response. The loss for the training samples should obviously go down and converge. If something else happens, reconsider the data encoding or the setup as a whole. The evaluation loss is the same score calculated for the evaluation data. It should behave very similar, if not, you might over-fit your model.\n",
    "\n",
    "Essentially the same applies for the **accuracy** which is calculated after each epoch, although it should obviously go up, not down. But when the curves for the training and evaluation data grow apart, you have to pay close attention to your results.\n",
    "\n",
    "Some **keywords to google** on: overfitting, underfitting, vanishing gradient, ?? blowup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['acc'], label='accuracy')\n",
    "if 'val_acc' in history.history:\n",
    "    plt.plot(history.history['val_acc'], label='eval_accuracy')\n",
    "plt.legend()#loc='lower right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='eval_loss')\n",
    "plt.legend()#loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 1.5, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### The SciKit-learn Pipeline\n",
    "putting it all into one big thing makes everything easier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# putting it all in a scikit pipeline for convenience (in notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "- point\n",
    "- point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pretty pictures -> getting deep!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "- point\n",
    "- point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's next?\n",
    "- Attention next big thing?\n",
    "- GAN (AlphaGo)\n",
    "- Curricula Training (Teacher/Student) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "Links to images used in this tutorial and some further reading.\n",
    "- [Online Book](http://neuralnetworksanddeeplearning.com/) on Neural Networks and Deep Learning by [Michael Nielsen](http://michaelnielsen.org/). *Very nice* introduction into the basic math behind ANNs (including SGD backprop training).\n",
    "- [Another Online Book](http://www.deeplearningbook.org/) on Deep Learning by Goodfellow, Bengio, and Courville\n",
    "- [TensorFlow](https://www.tensorflow.org/) (the machine learning thing from Google)\n",
    "- [Keras](https://keras.io/) as a very popular abstraction layer on top of TensorFlow, Theano, or (soon) CNTK; created by [Francois Chollet](https://twitter.com/fchollet)\n",
    "- Very simple and basic [implementation](https://gist.github.com/karpathy/d4dee566867f8291f086) of a character-level recurrent neural network that writes text by [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/).\n",
    "- Chris Olah wrote a [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) that roughly explains the intuition behind LSTMs in a nice visual way.\n",
    "- The setup of the first hands-on example of this tutorial was inspired by [this example](https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py).\n",
    "- [Paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) on why dropout helps to prevent over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>![noimg](images/test.png)</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "livereveal": {
   "height": 900,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "linear",
   "width": 1500
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "233px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "1584.38px",
    "right": "20px",
    "top": "106px",
    "width": "261px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
