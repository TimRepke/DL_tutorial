{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "Tim Repke, Julian Risch\n",
    "\n",
    "July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks\n",
    "- point\n",
    "- point\n",
    "- point\n",
    "- point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "some more on NN basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Formal Definition\n",
    "$y=wx+b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>![noimg](images/test.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Input to $k$-th neuron in layer $l$: $$z^l_k=\\sum^K_i a^{l-1}_i w^{l-1}_{ki}+b^l_k,$$ where $K$ is the number of neurons in the preceeding layer\n",
    "- Biases and weights can be represented as vectors and matrices\n",
    "- Weight matrix between layer $l-1$ and $l$: $$W^l=\n",
    " \\begin{pmatrix}\n",
    "  w^l_{11} & w^l_{12} & \\cdots & w^l_{1k} \\\\\n",
    "  w^l_{21} & w^l_{22} & \\cdots & w^l_{2k} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  w^l_{j1} & w^l_{j2} & \\cdots & w^l_{jk} \n",
    " \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "some more math in notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do they learn?\n",
    "- output neurons\n",
    "- error \"diff\" function (loss, cost, objective,...)\n",
    "- error back-propagated using gradient\n",
    "- weights and biases updated accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Types of Neural Networks\n",
    "- **ANN/NN:** Artificial Neural Networks *(1D input, what we just saw)*\n",
    "  - good for 1D input classification or regression\n",
    "  - learning embeddings (reduction into lower dimensional \"semantic\" space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **CNN:** Convolutional Neural Networks *(2D input)*\n",
    "  - 2D input classification; ideal for images, sometimes *fixed* length sequences\n",
    "  - image segmentation (masking areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **RNN:** Recurrent Neural Networks *(sequential input)*\n",
    "  - sequence to sequence models (i.e. translation, tagging, stock prognosis)\n",
    "  - sequence to scalar also possible (i.e. classifying entire sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks - Hands on!\n",
    "First, we load some general dependencies we may need all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Dataset\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "path = get_file(os.getcwd()+'/datasets/reuters.npz', origin='https://s3.amazonaws.com/text-datasets/reuters.npz')\n",
    "with np.load(path) as f:\n",
    "    reuters_data, labels = f['x'], f['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of samples:', reuters_data.shape)\n",
    "print('Part of the first sample:', reuters_data[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why are there only numbers? Isn't that supposed to be text? Indeed it is, but encoded by a dictionary, which we can load as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "path = get_file(os.getcwd()+'/datasets/reuters_word_index.json', origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.json')\n",
    "with open(path) as f:\n",
    "    reuters_words = json.load(f)\n",
    "    reuters_words_inverse = {v:k for k,v in reuters_words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dictionary size:\", len(reuters_words))\n",
    "print(\"Word index:\")\n",
    "pprint(dict(list(reuters_words.items())[:5]))\n",
    "print(\"\\nReverse word index:\")\n",
    "pprint(dict(list(reuters_words_inverse.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's \"decode\" some of those sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join([reuters_words_inverse[wi] for wi in reuters_data[42]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Getting to know the data is important.\n",
    "One interesting aspect is to look into the distribution of labels.\n",
    "A bias might require special attention during training.\n",
    "It also means, that we need to consider a strong bias in the evaluation later on, especially when creating the train/test split!\n",
    "You'll see some parameters later on, that lead back to the distribution of labels, i.e. `stratified` and `balanced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# a small pandas hack for \"simplicity\"\n",
    "label_distribution = pd.DataFrame({'labels':labels}).groupby('labels').labels.count()\n",
    "\n",
    "# make a barplot\n",
    "label_distribution.plot.bar(figsize=(12,5))\n",
    "\n",
    "# look at the bare numbers\n",
    "label_distribution.to_frame().T # remove the '.T' to see the full list vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Feel free to play with the data. \n",
    "- What's the distribution of words?\n",
    "- What's the distribution of words in different topics?\n",
    "- How long are the text samples?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### The easier way\n",
    "Keras has also some [built in](https://keras.io/datasets/) data loading functions that essentially do what we did above.\n",
    "That was just to show you how to go about things when you have you own data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "max_words = 50\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preparing the dataset\n",
    "There are two things we look at in this section. \n",
    "\n",
    "**Train/Test split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We split our data into two subsets.\n",
    "One, the bigger chunk, we use for training the mode, the other for testing it on.\n",
    "This way we have some data the model has never seen before.\n",
    "If it performs reasonable good classifying those, we can use it as an indicator, that it can generalise to other examples as well.\n",
    "However, there are several aspects one should consider depending on the dataset and task, like overlapping concepts or data.\n",
    "Here, we neglect all that for simplicity though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "At it's core, a neural network, as any other machine learning algorithm, is pure math. Text isn't. Thus we need to encode it in some way as numerical input as described down below.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Subsampling** data down to 4 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We just do that here in the example to make things easier and the evaluation less complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_labels = [4,11,16,19]\n",
    "selected_labels_index = {l:i for i,l in enumerate(selected_labels)}\n",
    "selection = [l in selected_labels for l in labels]\n",
    "reuters_data_sampled = reuters_data[selection]\n",
    "labels_sampled = labels[selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Original size of dataset:', len(labels))\n",
    "print('Size of subset for',len(selected_labels),'labels:', len(labels_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Splitting the data** using scikit-learn [utility functions](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note the parameter `stratify`, to which we hand our labels. This tells the function to not do the sampling completely random, but to make sure that samples from all classes are in both sets, as well as their relative distribution is about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reuters_data_sampled, labels_sampled,\n",
    "                                                    test_size=0.30, random_state=42,\n",
    "                                                    stratify=labels_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Size training set:', len(X_train))\n",
    "print('Size testing set:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Usually you would split your data into three parts:\n",
    "- one for **train**ing (usually the biggest chunk),\n",
    "- one for development **test**ing, \n",
    "- and one for a final **eval**uation.\n",
    "\n",
    "The last one you never look at or touch during your development and experiments. \n",
    "Only at the very end you will test the performance of what you determined to be your best model on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Encoding** the text-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In our first example with a simple neural network, we use a simple [bag-of-words (BoW)](https://en.wikipedia.org/wiki/Bag-of-words_model) encoding for our text.\n",
    "\n",
    "Keras provides [utility function](https://keras.io/preprocessing/text/#tokenizer), which we use to encode the text.\n",
    "It takes the first `max_words` entries in the dictionary and for each sample creates a vector of that length. \n",
    "Each position in that vector corresponds to a word's index in the dictionary and is set to 1 if it appears in the sample, 0 if not.\n",
    "In other modes, instead of setting a 1, some other score is used (i.e. number of occurences in the sample).\n",
    "\n",
    "Hints for experiments:\n",
    "- Try different vocab sizes\n",
    "- Try different modes\n",
    "- Look into the [implementation](https://github.com/fchollet/keras/blob/master/keras/preprocessing/text.py#L297) and do something like dropping stopwords (extremely frequently used words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words=5000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "X_train_tokenised = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "X_test_tokenised = tokenizer.sequences_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape, '| X_train_tokenised shape:', X_train_tokenised.shape)\n",
    "print('X_test  shape:', X_test.shape,  '| X_test_tokenised  shape:', X_test_tokenised.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Encoding** the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We interpret the output of the last layer of our network as the predicted label.\n",
    "We could naively assume, that we use one neuron that outputs the number to the corresponding class.\n",
    "However, it has proven to be significantly better to one-hot encode the labels, such that we have as many output neurons in the last layer as unique labels.\n",
    "Each output neuron corresponds to one label.\n",
    "Using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), we can interpret the out as a confidence score for that label - the higher (closer to 1), the more confident the network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_ = np.array([selected_labels_index[ty] for ty in y_train])\n",
    "y_test_ = np.array([selected_labels_index[ty] for ty in y_test])\n",
    "Y_train = to_categorical(y_train_, len(selected_labels))\n",
    "Y_test = to_categorical(y_test_, len(selected_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building the Neural Network\n",
    "One hidden layer with 512 neurons with ReLU [activation](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions); Softmax activation at the output layer; Optimisation using [Adam](https://arxiv.org/abs/1412.6980v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(selected_labels)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hints for experiments:\n",
    "- Try adding another layer\n",
    "- Check out the [documentation](https://keras.io/layers/core/)\n",
    "- Use different sizes of hidden layers\n",
    "- Use different weight initialisations\n",
    "- Use different [activation](https://keras.io/activations/) functions\n",
    "- Use different (or no) dropout\n",
    "- Use different [loss](https://keras.io/losses/) functions\n",
    "- Use different [optimisers](https://keras.io/optimizers/)\n",
    "\n",
    "Below you see a different syntax with some additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(max_words, activation='linear', input_dim=max_words, \n",
    "          kernel_initializer='he_uniform'),\n",
    "    Dense(2048, activation='relu', bias_regularizer=l2(0.01),\n",
    "          kernel_initializer='he_uniform', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(selected_labels), activation='softmax',\n",
    "          kernel_initializer='he_uniform')\n",
    "])\n",
    "\n",
    "model.compile(loss='msle',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Class weight computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As we have seen earlier, the distribution of samples across classes is somewhat unbalanced.\n",
    "This unbalance potentially causes to bias our classification model and there are different approaches to counteract that.\n",
    "One is to use weights during training, such that the error of samples from over-represented classes is virtually reduced by a factor and vice-versa amplified for samples of under-represented classes.\n",
    "For further reading, you could look up keywords like over- or under-sampling as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = {key: value for key, value in enumerate(\n",
    "                     compute_class_weight('balanced',\n",
    "                                          np.arange(len(selected_labels)),\n",
    "                                          np.array(y_train_)))}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are quite a few things going on here!\n",
    "Epochs sets the number of times we let the network see the entire training set.\n",
    "The batch size adjusts a number of things.\n",
    "Essentially, it grabs a batch of samples, does the forward passes, and only then does the back-propagation based on those gradients.\n",
    "This saves time, compared to calculating gradients and updating all variables for each training sample.\n",
    "The class weight can be used to amplify the error for samples from underrepresented classes.\n",
    "The validation split sets a small subset of *training* samples aside for testing along the training run.\n",
    "If we had a proper train/test/eval split, we would use the test split for that (or a portion for that, because calculating these metrics takes time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "history = model.fit(X_train_tokenised, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    #class_weight=class_weights,\n",
    "                    validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Keras has a simple evaluation function built into the model, which can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_tokenised, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We want to dig much deeper, so below you'll find a number of helpful [scikit-learn functions](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get model predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test_tokenised)\n",
    "y_pred = y_pred_proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **classification report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For each class, it shows the precision, recall and their harmonic mean (F1-score).\n",
    "The support shows, how many samples are in that particular class (based on test labels).\n",
    "We also calculate the accuracy.\n",
    "Both functions also allow normalisation to correct for class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_, y_pred, target_names=['Topic '+str(sl) for sl in selected_labels]))\n",
    "print(\"Accuracy: {0:.3f}\".format(accuracy_score(y_test_, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **confusion matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Based on the cofusion matrix you can figure out how the trained model makes mistakes. \n",
    "Vertically you can imagine the label of the correct class, horizontally that of the predicted class. \n",
    "The perfect classifier would only contain non-zero numbers along the diagonal.\n",
    "Considering the first row, you will see for samples of that topic, with what other topic they are confused with most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test_, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To get a more visually appealing result, you need a more code. Note that the normalisation was done *per class* for convenience, not globally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import cm as colm\n",
    "import matplotlib\n",
    "\n",
    "conf_matrix_full = confusion_matrix(y_test_, y_pred, labels=list(range(len(selected_labels))))\n",
    "\n",
    "matplotlib.rc('font', **{'size':12})\n",
    "plt.figure(figsize=(6,4))\n",
    "sub = plt.subplot(111)\n",
    "normed = (conf_matrix_full.T/conf_matrix_full.sum(axis=1)).T\n",
    "plt.imshow(normed, cmap=colm.Blues, vmax=normed.max()*1.4)\n",
    "sub.set_yticks(list(range(len(selected_labels))))\n",
    "sub.set_yticklabels(['Topic '+str(sl) for sl in selected_labels])\n",
    "sub.set_xticks(list(range(len(selected_labels))))\n",
    "sub.set_xticklabels(['Topic '+str(sl) for sl in selected_labels])\n",
    "\n",
    "for i in range(normed.shape[0]):\n",
    "    for j in range(normed.shape[1]):\n",
    "        v = normed.T[i][j]\n",
    "        c='%.2f'%v if v>0.005 else ''\n",
    "        sub.text(i, j, c, va='center', ha='center')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Area Under the Curve** and **Precision vs Recall curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As described earlier, we have an output neuron per class and interpret it's activation as the confidence.\n",
    "Above we made the simple assumption to just take the class where the neuron has the highest activation for each sample.\n",
    "We can also look at the precision recall curve, where we slowly increase the threshold of the confidence score we accept.\n",
    "Obviously, when we want a very high confidence close to 1, the precision will be (hopefully) very good.\n",
    "However, we may loose a lot of samples where there's more uncertainty, and thus the recall drops.\n",
    "\n",
    "The **precision recall curve describes the tradeoff** we can take.\n",
    "In the code below, you can print the variables returned by the sklearn function to see the range of thresholds.\n",
    "The AUC score essentially is just the area under the curve just described.\n",
    "A desireable model will have a high AUC and therefore the curve mostly high and stable.\n",
    "The perfect classifier which never fails would have a constant precision of 1 for all recall values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "plt.clf()\n",
    "for i, f in enumerate(['Topic '+str(sl) for sl in selected_labels]):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test_ == i, y_pred_proba[:, i])\n",
    "    auc_ = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=\"{}, AUC={:.4}\".format(f, auc_))\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.xlim(0, 1.1)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **training curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Remember that `history` variable we stored earlier during training? That contains all the values you see when you train the model with the verbose flag set to 1. We can also visualise it for convenience.\n",
    "\n",
    "Things to look out for are as follows: The **loss** is the value the optimiser aims to minimise *(also called objective, error,...)*. It describes, based on the loss function, how far off the current prediction is from the correct response. The loss for the training samples should obviously go down and converge. If something else happens, reconsider the data encoding or the setup as a whole. The evaluation loss is the same score calculated for the evaluation data. It should behave very similar, if not, you might over-fit your model.\n",
    "\n",
    "Essentially the same applies for the **accuracy** which is calculated after each epoch, although it should obviously go up, not down. But when the curves for the training and evaluation data grow apart, you have to pay close attention to your results.\n",
    "\n",
    "Some **keywords to google** on: overfitting, underfitting, vanishing gradient, ?? blowup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['acc'], label='accuracy')\n",
    "if 'val_acc' in history.history:\n",
    "    plt.plot(history.history['val_acc'], label='eval_accuracy')\n",
    "plt.legend()#loc='lower right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='eval_loss')\n",
    "plt.legend()#loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 1.5, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### The SciKit-learn Pipeline\n",
    "SciKit-learn offers a [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) interface, which allows you to **chain or combine multiple models and processing steps**. This simplifies your evaluation and experiments. Keras also implements an abstraction layer that works with this [API](https://keras.io/scikit-learn-api/). You may find this [in-depth tutorial](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.2-lesson/readme.html) on pipelines helpful to see what else you can do.\n",
    "\n",
    "The pipeline unfortunately only allows to transform the data/features, not the labels themselfs. Rather does it allow for downsampling. The example below is to small to really apply the Pipeline itself, but this interface shows its real power once you get into **parameter tuning**. Therefore, some of the utility functions are used instead. The [user guide](http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html) on model selection shows you some examples.\n",
    "\n",
    "Let's also assume, that we get raw text data and use SciKit-learn functions to transform the input.\n",
    "\n",
    "**Hints for experiments:**\n",
    "- Try to extend the example to do a [GridSearch](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) on the input layer size. You can add a parameter to the `build_model` function that sets the size. Click [here](http://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/) to cheat a bit.\n",
    "- Try different parameters for the `CountVectorizer` (i.e. set binary to False)\n",
    "- Replace the CountVectorizer with something else (i.e. TfidfVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "selected_labels = [4,11,16,19]\n",
    "max_words=5000\n",
    "num_folds = 5\n",
    "\n",
    "# pretend we have raw text data\n",
    "raw_reuters = [' '.join([reuters_words_inverse[wi] for wi in rd]) for rd in reuters_data]\n",
    "\n",
    "# subsample the data\n",
    "raw_reuters_sampled = [rr for i, rr in enumerate(raw_reuters) if labels[i] in selected_labels]\n",
    "labels_sampled = [l for l in labels if l in selected_labels]\n",
    "\n",
    "# helper function for our keras model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(max_words,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(len(selected_labels)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# encode raw text\n",
    "raw_reuters_encoded = CountVectorizer(max_features=max_words, binary=True).fit_transform(raw_reuters_sampled).todense()\n",
    "\n",
    "# prepare keras wrapper\n",
    "model = KerasClassifier(build_fn=build_model, epochs=5, batch_size=20, verbose=0)\n",
    "\n",
    "# set up kfold\n",
    "kfold = StratifiedKFold(n_splits=num_folds, random_state=42)\n",
    "\n",
    "# do the computations\n",
    "results = cross_val_score(model, raw_reuters_encoded, labels_sampled, cv=kfold)\n",
    "print(results.mean())\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "- point\n",
    "- point\n",
    "\n",
    "motivation: für daten die nur im kontext sinn machen. NN soll die dann acuh als kontext verarbeiten können (locality assumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pretty pictures -> getting deep!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CNN - Hands on!\n",
    "In this section:\n",
    "- encode text as 2D input\n",
    "- usage of keras embedding and convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Data Preparation\n",
    "We already did that earlier on. In case you start the tutorial from here, you can execute the cell below to get all you need from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from keras.utils.data_utils import get_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "path_data = get_file(os.getcwd()+'/datasets/reuters.npz', \n",
    "                origin='https://s3.amazonaws.com/text-datasets/reuters.npz')\n",
    "path_dict = get_file(os.getcwd()+'/datasets/reuters_word_index.json', \n",
    "                     origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.json')\n",
    "\n",
    "\n",
    "with np.load(path_data) as f_data, open(path_dict) as f_dict:\n",
    "    reuters_data, labels = f_data['x'], f_data['y']\n",
    "    reuters_words = json.load(f_dict)\n",
    "    reuters_words_inverse = {v:k for k,v in reuters_words.items()}\n",
    "    \n",
    "    # downsample to a few topics\n",
    "    selected_labels = [4,11,16,19]\n",
    "    selected_labels_index = {l:i for i,l in enumerate(selected_labels)}\n",
    "    selection = [l in selected_labels for l in labels]\n",
    "    reuters_data_sampled = reuters_data[selection]\n",
    "    labels_sampled = labels[selection]\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reuters_data_sampled, labels_sampled,\n",
    "                                                    test_size=0.20, random_state=42,\n",
    "                                                    stratify=labels_sampled)\n",
    "    # one-hot encode the labels\n",
    "    y_train_ = np.array([selected_labels_index[ty] for ty in y_train])\n",
    "    y_test_ = np.array([selected_labels_index[ty] for ty in y_test])\n",
    "    Y_train = to_categorical(y_train_, len(selected_labels))\n",
    "    Y_test = to_categorical(y_test_, len(selected_labels))\n",
    "    \n",
    "    # calculate class weights\n",
    "    class_weights = {key: value for key, value in enumerate(\n",
    "        compute_class_weight('balanced', np.arange(len(selected_labels)), np.array(y_train_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoding the Text\n",
    "- Naive: \"strech out\" the bag-of-words\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\n",
    "\\text{\"I like Rheinsberg\"}\n",
    "\\xrightarrow{to num}\n",
    "\\begin{bmatrix}0&3&1&4\\end{bmatrix}\n",
    "\\xrightarrow{one hot}\n",
    "\\begin{bmatrix}\n",
    "0&0&0&0\\\\\n",
    "0&0&1&0\\\\\n",
    "0&0&0&0\\\\\n",
    "0&1&0&0\\\\\n",
    "0&0&0&1\\\\\n",
    "\\end{bmatrix}$\n",
    "- towards state-of-the-art: word embeddings\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\xrightarrow{embedding}\n",
    "\\begin{bmatrix}\n",
    "0.11&0.42&-0.44&0.12\\\\\n",
    "0.23&0.50&0.35&-0.33\\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "some text about previous slide\n",
    "- [Keras Example](https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py)\n",
    "- [Keras Tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Reducing the Vocabulary**\n",
    "\n",
    "The unique number of words used can become very large and it makes sense to reduce it.\n",
    "\n",
    "Feel free to adapt the later model accordingly and see how a reduced vocabulary helps to improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is done for a number of reasons. Most importantly we don't have a large amount of training data, so the model should learn from things that are most likely to reappear during testing. We can just **remove very uncommon words**, as well as so-called **stop words** (like \"and\", \"the\", \"or\"), which appear very often and don't carry much meaning. The word usage frequence in natural languages usually follows [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "Note, that **this is optional**. The amount of words removed from the head and tail of the distribution depends on the data and problem at hand and probably requires some experimentation. The [TF-IDF Scores](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), can be helpful for that, as they give higher scores to words, which help distinguish documents, and low scores for those, that frequently appear in a lot of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of words (train):\", np.array([a for b in X_train for a in b]).shape)\n",
    "print(\"Number of unique words (train):\", np.unique([a for b in X_train for a in b]).shape)\n",
    "print(\"Number of words (test):\", np.array([a for b in X_test for a in b]).shape)\n",
    "print(\"Number of unique words (test):\", np.unique([a for b in X_test for a in b]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The code cells below allow you to count the frequency of each word and remove all but the most frequent (`vocabulary_size`) words from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "word_counts = {}\n",
    "for doc in X_train: \n",
    "    for term in doc:\n",
    "        word_counts[term] = word_counts.get(term, 0) + 1\n",
    "\n",
    "keep_words = [x[0] for x in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:vocabulary_size]]\n",
    "\n",
    "X_train_reduced = [[t for t in doc if t in keep_words] for doc in X_train]\n",
    "X_test_reduced = [[t for t in doc if t in keep_words] for doc in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we managed to reduce the vocabulary size significantly, but the total number of words in the texts did not change too much. Note also, that some words in the test set are missing in our training set. That is a common real world scenario, which the model has to be able to cope with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of words (train):\", np.array([a for b in X_train_reduced for a in b]).shape)\n",
    "print(\"Number of unique words (train):\", np.unique([a for b in X_train_reduced for a in b]).shape)\n",
    "print(\"Number of words (test):\", np.array([a for b in X_test_reduced for a in b]).shape)\n",
    "print(\"Number of unique words (test):\", np.unique([a for b in X_test_reduced for a in b]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Normalising Text Length**\n",
    "\n",
    "The input texts all have different length but the neural network has a fixed size. We limit the size to a fixed length, long texts are cut, short texts are padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "maxlen = 400\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print(\"Shape of train:\", X_train_padded.shape)\n",
    "print(\"Shape of test:\", X_test_padded.shape)\n",
    "X_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "vocab_size = np.unique([a for b in X_train_padded for a in b]).shape[0]\n",
    "embedding_size = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(selected_labels)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_padded, Y_train,\n",
    "                    batch_size=20,\n",
    "                    epochs=5,\n",
    "                    class_weights=class_weights,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this case you probably won't find it useful, but generally you can use **embeddings as dimensionality reduction**. Word embeddings specifically have the nice property of \"placing\" semantically similar words nearby in the embedding space. Since you can have a look at each layer output of the sequential keras model, you can get the embedding of what it learned in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# embed the first sentence\n",
    "to_embed = X_train_padded[:1]\n",
    "\n",
    "# or something else you did yourself\n",
    "to_embed = np.zeros((1,400))\n",
    "to_embed[0][399] = 50\n",
    "\n",
    "embed = K.function([model.layers[0].input], [model.layers[0].output])\n",
    "embedded = embed([to_embed])[0]\n",
    "\n",
    "print(\"size of vocabulary:\", vocab_size)\n",
    "print(\"size of embedding:\", embedding_size)\n",
    "print(\"shape of embedded text:\", embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing the model\n",
    "Just as before, we can evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test_padded)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "print()\n",
    "print(classification_report(y_test_, y_pred, target_names=['Topic '+str(sl) for sl in selected_labels]))\n",
    "print(\"Accuracy: {0:.3f}\".format(accuracy_score(y_test_, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It is also possible to use pre-trained word embeddings. Those are trained on very large text corpora and therefore have a large vocabulary, from which you can infer meaningful word embeddings. There is a [Keras Tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) on how to use these embeddings directly in your model.\n",
    "\n",
    "Note, that you can use such embeddings as is (fixed) or continue updating the weights accordingly during training to enforce a bias towards the wording used in the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1. Download: [GloVe](https://nlp.stanford.edu/projects/glove/) [word embedding](http://nlp.stanford.edu/data/glove.6B.zip) trained on Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, **Warning: 822 MB download**, almost 3GB unpacked)\n",
    "2. unpack to `datasets/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# load the word vectors into memory\n",
    "# store them in a lookup dictionary\n",
    "embeddings_index = {}\n",
    "glove_embedding_size = 100\n",
    "with open(os.getcwd()+'/datasets/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# map the pretrained vectors to the dictionary we already have\n",
    "embedding_matrix = np.zeros((len(reuters_words) + 1, glove_embedding_size))\n",
    "for word, i in reuters_words.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(reuters_words) + 1,\n",
    "                    glove_embedding_size,\n",
    "                    # set the weights (pretrained vectors)\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    # flip to true to \"overfit\" to vocab at hand\n",
    "                    trainable=False)) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(selected_labels)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# now you can compile the model and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "- point\n",
    "- point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RNN - Hands on!\n",
    "In keras, we would just add another line of code.\n",
    "\n",
    "Let's switch it up by going down one level and look at TensorFlow!\n",
    "\n",
    "This is inspired by [this blog post](https://danijar.com/variable-sequence-lengths-in-tensorflow/), [full code](https://gist.github.com/danijar/3f3b547ff68effb03e20c470af22c696).\n",
    "\n",
    "Or this?\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py\n",
    "\n",
    "https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "\n",
    "http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the cell below. There are some additional dependencies we need from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "# choose the embedding size\n",
    "glove_embedding_size = [50,100,200,300][1]\n",
    "\n",
    "# select topics of interest\n",
    "selected_labels = [4,11,16,19]\n",
    "selected_labels_index = {l:i for i,l in enumerate(selected_labels)}\n",
    "\n",
    "\n",
    "path_data = get_file(os.getcwd()+'/datasets/reuters.npz', \n",
    "                origin='https://s3.amazonaws.com/text-datasets/reuters.npz')\n",
    "path_dict = get_file(os.getcwd()+'/datasets/reuters_word_index.json', \n",
    "                     origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.json')\n",
    "path_glove = os.getcwd()+'/datasets/glove.6B.'+str(glove_embedding_size)+'d.txt'\n",
    "path_data_tfr = os.getcwd()+'/datasets/reuters.tfrecords'\n",
    "    \n",
    "# don't need to do that if the file already exists\n",
    "if not os.path.isfile(path_data_tfr):\n",
    "    print(\"tfrecords file does not exist yet, writing it!\")\n",
    "    with np.load(path_data) as f_data:\n",
    "        # load data from old format file\n",
    "        reuters_data, labels = f_data['x'], f_data['y']\n",
    "        \n",
    "        # this writes the training data into a TFRecords protobuf file\n",
    "        writer = tf.python_io.TFRecordWriter(path_data_tfr)\n",
    "        idx = np.arange(len(reuters_data))\n",
    "        np.random.shuffle(idx)\n",
    "        for i in idx:\n",
    "            features = np.array(reuters_data[i])\n",
    "            label = labels[i]\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "                    'text': tf.train.Feature(int64_list=tf.train.Int64List(value=features.astype(\"int64\"))),\n",
    "            }))\n",
    "            serialized = example.SerializeToString()\n",
    "\n",
    "            writer.write(serialized)\n",
    "        writer.close() \n",
    "else:\n",
    "    print(\"tfrecords file already exists.\")\n",
    "    \n",
    "with open(path_dict) as f_dict, open(path_glove) as f_glove:\n",
    "    reuters_words = json.load(f_dict)\n",
    "    reuters_words_inverse = {v:k for k,v in reuters_words.items()}\n",
    "    \n",
    "    # loading embeddings takes a while\n",
    "    # in case you just run this to reset X and y, this will skip reloading embeddings\n",
    "    #embeddings_index = {}\n",
    "    #del embeddings_index\n",
    "    try:\n",
    "        embeddings_index\n",
    "        print('word vectors already loaded!')\n",
    "        # to reload trained vectors, uncomment the next line\n",
    "        #qwerty\n",
    "    except NameError:\n",
    "        print('word vectors not loaded yet, doing that now.')\n",
    "        # build embedding index\n",
    "        embeddings_index = {}\n",
    "        for line in f_glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "        # map the pretrained vectors to the dictionary we already have\n",
    "        embedding_matrix = np.zeros((len(reuters_words) + 1, glove_embedding_size))\n",
    "        for word, i in reuters_words.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Initial Notes**\n",
    "\n",
    "TensorFlow builds a graph of dependencies, which is used to efficiently schedule computations. This graph \"lives outside\" a session and has a \"memory\". User-defined data can be put in placeholders that are filled in a session, other global variables need to be initialised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this resets the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# at the beginning of a session, you need to initialise global variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Define placeholders**\n",
    "\n",
    "Those variables will be set and used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch size will be set later\n",
    "#batch_size_placeholder = tf.placeholder(tf.int32)\n",
    "batch_size = 2\n",
    "\n",
    "# data for a batch\n",
    "#tf.placeholder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Embedding Layer**\n",
    "\n",
    "The embedding layer is essentially just a matrix, which we loaded earlier and now put in the tensorflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the embedding layer\n",
    "embedding_layer = tf.Variable(tf.constant(0.0, shape=list(embedding_matrix.shape)),\n",
    "                              trainable=False, name=\"embedding\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, list(embedding_matrix.shape))\n",
    "embedding_init = embedding_layer.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**> Queue for input data**\n",
    "\n",
    "We define some functions that read and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filereader(input_files):\n",
    "    if type(input_files) != 'list':\n",
    "        input_files = [input_files]\n",
    "    filename_queue = tf.train.string_input_producer(input_files, num_epochs=None)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'text': tf.VarLenFeature(tf.int64)\n",
    "        })\n",
    "    label = features['label']\n",
    "    text = tf.sparse_tensor_to_dense(features['text'])\n",
    "    return label, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "label, text = filereader(path_data_tfr)\n",
    "print(label)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        print(sess.run([label, text]))\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "frame_size = 64\n",
    "num_hidden = 200\n",
    "learning_rate = 0.003\n",
    "batch_size = 50\n",
    "selected_labels = [4,11,16,19]\n",
    "out_size = 46 # = max label + 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "label, text = filereader(path_data_tfr)\n",
    "\n",
    "embedding_layer = tf.Variable(tf.constant(0.0, shape=embedding_matrix.shape),\n",
    "                              trainable=False, name=\"embedding\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, embedding_matrix.shape)\n",
    "embedding_init = embedding_layer.assign(embedding_placeholder)\n",
    "\n",
    "text_batch, labels_batch = tf.train.batch(\n",
    "    [text, label], \n",
    "    batch_size=batch_size,\n",
    "    capacity=200,\n",
    "    dynamic_pad=True)\n",
    "\n",
    "label_filter = [tf.equal(labels_batch, selected_labels[0])]\n",
    "for i, sl in enumerate(selected_labels[1:]):\n",
    "    label_filter.append(tf.logical_or(label_filter[i], tf.equal(labels_batch, sl)))\n",
    "\n",
    "reduced_text_batch = tf.gather(text_batch, tf.reshape(tf.where(label_filter[-1]),[-1]))\n",
    "reduced_labels_batch = tf.gather(labels_batch, tf.reshape(tf.where(label_filter[-1]),[-1]))\n",
    "\n",
    "embedded = tf.gather(embedding_layer, reduced_text_batch)\n",
    "\n",
    "labels_batch_ = tf.one_hot(reduced_labels_batch, out_size)\n",
    "lengths = tf.cast(tf.reduce_sum(tf.sign(reduced_text_batch), axis=1), tf.int32)\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(\n",
    "    tf.nn.rnn_cell.GRUCell(num_hidden),\n",
    "    embedded,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=lengths\n",
    ")\n",
    "\n",
    "current_batch_length = tf.shape(output)[0]\n",
    "current_batch_width = tf.shape(output)[1]\n",
    "current_output_size = tf.shape(output)[2]\n",
    "\n",
    "last_relevant_index = tf.range(0, current_batch_length) * current_batch_width + (lengths - 1)\n",
    "flat_output = tf.reshape(output, [-1, current_output_size])\n",
    "last_out = tf.gather(flat_output, last_relevant_index)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([num_hidden, out_size], stddev=0.01))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(last_out, weight) + bias)\n",
    "\n",
    "cross_entropy_loss = -tf.reduce_sum(labels_batch_ * tf.log(prediction))\n",
    "\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(cross_entropy_loss)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding_matrix})\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        for _ in range(10): # num batches\n",
    "            sess.run([train_op, current_batch_length,current_batch_width])\n",
    "            print(str(current_batch_length.eval())+'('+str(current_batch_length.eval())+').', end='')\n",
    "        \n",
    "        error = sess.run(cross_entropy_loss)\n",
    "        print('Epoch {:2d} error {:5.2f}%'.format(epoch + 1, 100 * error))\n",
    "        \n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "- DL nicht allheilmittel\n",
    "- GIGO (garbage in garbage out)\n",
    "- feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's next?\n",
    "- Attention next big thing?\n",
    "- GAN (AlphaGo)\n",
    "- Curricula Training (Teacher/Student) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other example\n",
    "Check out the other two notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "Links to images used in this tutorial and some further reading.\n",
    "- [Online Book](http://neuralnetworksanddeeplearning.com/) on Neural Networks and Deep Learning by [Michael Nielsen](http://michaelnielsen.org/). *Very nice* introduction into the basic math behind ANNs (including SGD backprop training).\n",
    "- [Another Online Book](http://www.deeplearningbook.org/) on Deep Learning by Goodfellow, Bengio, and Courville\n",
    "- [TensorFlow](https://www.tensorflow.org/) (the machine learning thing from Google)\n",
    "- [Keras](https://keras.io/) as a very popular abstraction layer on top of TensorFlow, Theano, or (soon) CNTK; created by [Francois Chollet](https://twitter.com/fchollet)\n",
    "- Very simple and basic [implementation](https://gist.github.com/karpathy/d4dee566867f8291f086) of a character-level recurrent neural network that writes text by [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/).\n",
    "- Chris Olah wrote a [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) that roughly explains the intuition behind LSTMs in a nice visual way.\n",
    "- The setup of the first hands-on example of this tutorial was inspired by [this example](https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py).\n",
    "- [Paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) on why dropout helps to prevent over-fitting.\n",
    "- A [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on types of RNNs and their effectiveness by Andrej Karpathy.\n",
    "- Very helpful [post](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/) on hidden TensorFlow features, which may save you a lot hot head scratching by Denny Britz.\n",
    "- This [blog post](https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/) describes some perks of using protobuf queues in TensorFlow.\n",
    "- Your Tensorflow code is slow even on a GPU? [This post](https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/) might help you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Talk: NNs for Classification (15') - 9:00\n",
    "  - NN are nerons in layers -> what other buzzwords do you know?\n",
    "  - from perceptron model to layers\n",
    "  - weights as matrices, forward pass as simple lin-alg, error at end -> back propagation\n",
    "  - mathematical model in tensorflow, efficiently distribute computations (tensor dependency graph)\n",
    "  - abstraction of that with keras\n",
    "- NN hands on (20') - 9:15\n",
    "  - dependencies, data, split&encoding\n",
    "  - NN architecture (batch training, cost functions, learning rate, optimiser)\n",
    "  - evaluation strategies\n",
    "- Talk: CNN (10') - 9:35\n",
    "  - motivation: locality assumption (some data only usefil in context, CNN allow NN to project context)\n",
    "  - 1-hot vs embedding (sparse vs abstract), what is embedding\n",
    "- CNN hands on (15') - 9:45\n",
    "  - preprocessing (cutting/padding, dict limit, map to embedding)\n",
    "  - max pooling, feature maps (deeper, wider nets)\n",
    "- Talk: RNN (10') - 10:00\n",
    "  - motivation: sequential data!\n",
    "  - GRU/LSTM cells as \"gates\" that let information through or not\n",
    "- RNN hands on (10') - 10:10\n",
    "- Conclusion (5') - 10:20\n",
    "  - DL not magic bullet!\n",
    "  - GIGO\n",
    "  - feature engineering shifts, now it's getting data and trying new ways to represent it\n",
    "- play time (open end) - 10:25\n",
    "  - experiment with hints, look at other datasets (see other notebooks)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "livereveal": {
   "height": 900,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "linear",
   "width": 1500
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "233px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "721px",
    "left": "1600.38px",
    "right": "20px",
    "top": "134px",
    "width": "292px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
